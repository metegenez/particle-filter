{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_sigma = np.sqrt(0.5);\n",
    "process_noise = np.sqrt(0.1);\n",
    "sensor_locations = np.array([[0,0],[3000,3000]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bearing():\n",
    "    def __init__(self,sensor_no,bearing,time):\n",
    "        self.sensor_no = sensor_no #can be 0 or 1\n",
    "        self.sensor_location = np.array([[0,0],[3000,3000]])[sensor_no]\n",
    "        self.bearing = bearing\n",
    "        self.time = time\n",
    "        self.track_id = 0 #for multiple target evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_positions = np.array([100,200])\n",
    "initial_velocities = np.array(5) #m/s\n",
    "initial_orientation = np.array([335])\n",
    "dx = initial_velocities * np.cos(np.deg2rad(90 - initial_orientation))\n",
    "dy = initial_velocities * np.sin(np.deg2rad(90 - initial_orientation))\n",
    "ts = 1\n",
    "print(dx)\n",
    "print(dy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = lambda Ts: np.array([[1, 0, Ts, 0],[0, 1, 0, Ts], [0, 0, 1, 0],[0, 0, 0, 1]])\n",
    "B = lambda ts: np.concatenate(((ts**2)*0.5*np.eye(2), ts * np.eye(2)), axis = 0)\n",
    "aci = lambda konum, sensor_no: np.rad2deg(np.arctan2(konum[0] - sensor_locations[sensor_no][0], konum[1] - sensor_locations[sensor_no][1])) #x vey y ozellikle boyle (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_time_array = np.linspace(0,100,100 / ts)\n",
    "time_array = np.sort(np.concatenate((temp_time_array, temp_time_array + np.random.rand(1,100) * 1/2), axis=None))\n",
    "car_traj = [np.concatenate((initial_positions, dx, dy), axis=None)]\n",
    "bearings = [bearing(0,aci(car_traj[0],0),time_array[0])]\n",
    "for i in range(1,len(time_array)):\n",
    "    car_traj.append((np.matmul(A(time_array[i]-time_array[i-1]),car_traj[-1].T).T + np.matmul(B(time_array[i]-time_array[i-1]), np.random.rand(2,1)).T * process_noise)[0])\n",
    "coordinates = np.array(car_traj)[:,0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(car_traj)):\n",
    "    a = np.random.choice([False,True], p = [0.5,0.5])\n",
    "    if a:\n",
    "        bearings.append(bearing(0,aci(car_traj[i],0),time_array[i]))\n",
    "    else:\n",
    "        bearings.append(bearing(1,aci(car_traj[i],1),time_array[i])) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearings[0].bearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(coordinates[:,0],coordinates[:,1])\n",
    "plt.scatter(sensor_locations[0][0],sensor_locations[0][1], c = \"red\")\n",
    "plt.scatter(sensor_locations[1][0],sensor_locations[1][1], c = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Filtering\n",
    "We assume that we know initial point of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "h = lambda particles, bearing: 90 - (torch.atan2(particles[:,1] - bearing.sensor_location[1],particles[:,0] - bearing.sensor_location[0])* 57.295779513)\n",
    "initial_state = torch.Tensor(car_traj[0])\n",
    "initial_covarience_matrix = torch.diag(torch.Tensor([10**2,10**2,10,10]))\n",
    "initial_sampling = torch.distributions.MultivariateNormal(loc= initial_state,covariance_matrix = initial_covarience_matrix)\n",
    "particles = torch.squeeze(initial_sampling.expand([N, 1]).sample())\n",
    "weights = torch.ones(N) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_resample(weights):\n",
    "    \"\"\" Performs the residual resampling algorithm used by particle filters.\n",
    "    Based on observation that we don't need to use random numbers to select\n",
    "    most of the weights. Take int(N*w^i) samples of each particle i, and then\n",
    "    resample any remaining using a standard resampling algorithm [1]\n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : list-like of float\n",
    "        list of weights as floats\n",
    "    Returns\n",
    "    -------\n",
    "    indexes : ndarray of ints\n",
    "        array of indexes into the weights defining the resample. i.e. the\n",
    "        index of the zeroth resample is indexes[0], etc.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. S. Liu and R. Chen. Sequential Monte Carlo methods for dynamic\n",
    "       systems. Journal of the American Statistical Association,\n",
    "       93(443):1032â€“1044, 1998.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(weights)\n",
    "    #indexes = np.zeros(N, 'i')\n",
    "    indexes=torch.zeros(N,dtype=torch.int32)\n",
    "    # take int(N*w) copies of each weight, which ensures particles with the\n",
    "    # same weight are drawn uniformly\n",
    "    #num_copies = (np.floor(N*np.asarray(weights))).astype(int)\n",
    "    num_copies = (torch.floor(N*torch.as_tensor(weights))).int()\n",
    "    k = 0\n",
    "    for i in range(N):\n",
    "        for _ in range(num_copies[i]): # make n copies\n",
    "            indexes[k] = i\n",
    "            k += 1\n",
    "\n",
    "    # use multinormal resample on the residual to fill up the rest. This\n",
    "    # maximizes the variance of the samples\n",
    "    residual = weights - num_copies     # get fractional part\n",
    "    residual /= sum(residual)           # normalize\n",
    "    #cumulative_sum = np.cumsum(residual)\n",
    "    cumulative_sum = torch.cumsum(residual,dim=0)\n",
    "    cumulative_sum[-1] = 1. # avoid round-off errors: ensures sum is exactly one\n",
    "    indexes[k:N] = np.searchsorted(cumulative_sum, random(N-k))\n",
    "\n",
    "    return indexes\n",
    "\n",
    "\n",
    "\n",
    "def stratified_resample(weights):\n",
    "    \"\"\" Performs the stratified resampling algorithm used by particle filters.\n",
    "    This algorithms aims to make selections relatively uniformly across the\n",
    "    particles. It divides the cumulative sum of the weights into N equal\n",
    "    divisions, and then selects one particle randomly from each division. This\n",
    "    guarantees that each sample is between 0 and 2/N apart.\n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : list-like of float\n",
    "        list of weights as floats\n",
    "    Returns\n",
    "    -------\n",
    "    indexes : ndarray of ints\n",
    "        array of indexes into the weights defining the resample. i.e. the\n",
    "        index of the zeroth resample is indexes[0], etc.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(weights)\n",
    "    # make N subdivisions, and chose a random position within each one\n",
    "    #positions = (random(N) + range(N)) / N\n",
    "    positions = (torch.rand(N) + torch.arange(N)) / N\n",
    "\n",
    "    #indexes = np.zeros(N, 'i')\n",
    "    indexes=torch.zeros(N,dtype=torch.int32)\n",
    "    #cumulative_sum = np.cumsum(weights)\n",
    "    cumulative_sum = torch.cumsum(weights,dim=0)\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def systematic_resample(weights):\n",
    "    \"\"\" Performs the systemic resampling algorithm used by particle filters.\n",
    "    This algorithm separates the sample space into N divisions. A single random\n",
    "    offset is used to to choose where to sample from for all divisions. This\n",
    "    guarantees that every sample is exactly 1/N apart.\n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : list-like of float\n",
    "        list of weights as floats\n",
    "    Returns\n",
    "    -------\n",
    "    indexes : ndarray of ints\n",
    "        array of indexes into the weights defining the resample. i.e. the\n",
    "        index of the zeroth resample is indexes[0], etc.\n",
    "    \"\"\"\n",
    "    N = len(weights)\n",
    "\n",
    "    # make N subdivisions, and choose positions with a consistent random offset\n",
    "    #positions = (random() + np.arange(N)) / N\n",
    "    positions = (torch.rand(1) + torch.arange(N)) / N\n",
    "\n",
    "    #indexes = np.zeros(N, 'i')\n",
    "    indexes=torch.zeros(N,dtype=torch.int32)\n",
    "    #cumulative_sum = np.cumsum(weights)\n",
    "    cumulative_sum = torch.cumsum(weights,dim=0)\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(particles, weights, bearing, sigma, h_func):\n",
    "    v = bearing.bearing - h_func(particles,bearing)\n",
    "    v[(v < -170) * (v > -190)] += 180 \n",
    "    v[(v < -350) * (v > -370)] += 360\n",
    "    v[(v < 190) * (v > 170)] -= 180\n",
    "    v[(v < 370) * (v > 350)] -= 360\n",
    "    weigths = weights * torch.distributions.normal.Normal(0,sigma).log_prob(v).exp()  \n",
    "    weights = weigths + torch.Tensor([1.e-300])     # avoid round-off to zero\n",
    "    weights = weigths / torch.sum(weights) # normalize\n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(particles, ts=torch.Tensor([1.])):\n",
    "    \"\"\" \n",
    "    One step propagation. It is linear in Bearing-Only-Tracking.\n",
    "    \"\"\"\n",
    "    particles = torch.matmul(particles,torch.t(torch.tensor(A(ts)).float())) \n",
    "    particles += torch.transpose(torch.mm(torch.Tensor(B(time_array[i]-time_array[i-1])),torch.empty([2,len(particles)]).normal_(mean=0,std=process_noise)),0,1)\n",
    "    \n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(particles, weights):\n",
    "    \"\"\"returns mean and variance of the weighted particles\"\"\"\n",
    "    mean = torch.sum(torch.mul(particles,weights[None].transpose_(0, 1)),dim=0)\n",
    "    var = torch.sum(torch.mul((particles - mean).pow(2),weights[None].transpose_(0, 1)),dim=0)\n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neff(weights):\n",
    "    return 1. / torch.dot(weights,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_from_index(particles, weights, indexes):\n",
    "    particles[:] = particles[indexes.long()]\n",
    "    weights[:] = weights[indexes.long()]\n",
    "    weights = torch.full(weights.shape, 1.0 / len(weights))\n",
    "    print(weights)\n",
    "    return weights,particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "takip = []\n",
    "plot = True\n",
    "old_time = bearings[0].time\n",
    "for bearing in bearings[1:]:\n",
    "    particles = predict(particles, ts = torch.Tensor([bearing.time - old_time]))\n",
    "    weights = update(particles, weights, bearing, sigma = measurement_sigma, h_func = h)\n",
    "    # resample if too few effective particles\n",
    "    # resample if too few effective particles\n",
    "    \n",
    "    \n",
    "    if neff(weights) < N/2:\n",
    "        indexes = systematic_resample(weights)\n",
    "        #print(\"--------------------\")\n",
    "        #print(weights)\n",
    "        weights,particles=resample_from_index(particles, weights, indexes)\n",
    "        #print(\"----------------------\")\n",
    "        #print(weights)\n",
    "        #print(\"------------------------\")\n",
    "        #assert torch.allclose(weights, torch.full(1/N))\n",
    "        #assert torch.allclose(weights, torch.full(weights.shape,1/N))\n",
    "        \n",
    "    mean, var = estimate(particles, weights)\n",
    "    takip.append(mean)\n",
    "    if plot:\n",
    "            alpha = .20\n",
    "            if N > 5000:\n",
    "                alpha *= np.sqrt(5000)/np.sqrt(N)           \n",
    "            plt.scatter(particles[:, 0], particles[:, 1], \n",
    "                        alpha=alpha, color='g')\n",
    "    \n",
    "    plt.scatter(coordinates[:,0], coordinates[:,1], marker='+',color='y', s=180, lw=3)\n",
    "    plt.scatter(mean[0], mean[1], marker='+',color='r', s=180, lw=3)\n",
    "    old_time = bearing.time\n",
    "    \n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import PillowWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles[:,0].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
