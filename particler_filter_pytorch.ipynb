{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #1.3.1\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from celluloid import Camera\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bearing():\n",
    "    def __init__(self,sensor_no,bearing,time):\n",
    "        self.sensor_no = sensor_no #can be 0 or 1\n",
    "        self.sensor_location = np.array([[0,0],[3000,3000]])[sensor_no]\n",
    "        self.bearing = bearing\n",
    "        self.time = time\n",
    "        self.track_id = 0 #for multiple target evaluations\n",
    "A = lambda Ts: np.array([[1, 0, Ts, 0],[0, 1, 0, Ts], [0, 0, 1, 0],[0, 0, 0, 1]])\n",
    "B = lambda ts: np.concatenate(((ts**2)*0.5*np.eye(2), ts * np.eye(2)), axis = 0)\n",
    "aci = lambda konum, sensor_no: np.rad2deg(np.arctan2(konum[0] - sensor_locations[sensor_no][0], konum[1] - sensor_locations[sensor_no][1])) #x vey y ozellikle boyle (x,y)\n",
    "def generate_a_target(measurement_sigma, process_noise, ts, sensor_locations = np.array([[0,0],[3000,3000]])):\n",
    "    initial_positions = np.random.rand(2) *3000\n",
    "    initial_velocities = np.random.rand(1) *10 #m/s\n",
    "    initial_orientation = np.random.rand(1) * 360\n",
    "    print(initial_orientation)\n",
    "    dx = initial_velocities * np.cos(np.deg2rad(90 - initial_orientation))\n",
    "    dy = initial_velocities * np.sin(np.deg2rad(90 - initial_orientation))\n",
    "    temp_time_array = np.linspace(0,100,100 / ts)\n",
    "    time_array = np.sort(np.concatenate((temp_time_array, temp_time_array + np.random.rand(1,int(100 / ts))), axis=None))\n",
    "    car_traj = [np.concatenate((initial_positions, dx, dy), axis=None)]\n",
    "    bearings = [bearing(0,aci(car_traj[0],0),time_array[0])]\n",
    "    for i in range(1,len(time_array)):\n",
    "        car_traj.append((np.matmul(A(time_array[i]-time_array[i-1]),car_traj[-1].T).T + np.matmul(B(time_array[i]-time_array[i-1]), np.random.normal(0, process_noise, (2,1))).T * process_noise)[0])\n",
    "    coordinates = np.array(car_traj)[:,0:2]\n",
    "    for i in range(1,len(car_traj)):\n",
    "        a = np.random.choice([False,True], p = [0.5,0.5])\n",
    "        if a:\n",
    "            bearings.append(bearing(0,aci(car_traj[i],0) + torch.empty(1).normal_(mean=0,std=measurement_sigma),time_array[i]))\n",
    "        else:\n",
    "            bearings.append(bearing(1,aci(car_traj[i],1) + torch.empty(1).normal_(mean=0,std=measurement_sigma),time_array[i])) \n",
    "    return car_traj, bearings, coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Create\n",
    "Single-target scenerio. If you can use \"thread\" (known associations), just customize \"bearing\" object with \"track_id\" and create a model accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_sigma = np.sqrt(1);\n",
    "process_noise = np.sqrt(0.5);\n",
    "sensor_locations = np.array([[0,0],[3000,3000]])\n",
    "ts = 1/2\n",
    "car_traj, bearings, coordinates = generate_a_target(measurement_sigma, process_noise, ts)\n",
    "plt.scatter(coordinates[:,0],coordinates[:,1])\n",
    "plt.scatter(sensor_locations[0][0],sensor_locations[0][1], c = \"red\")\n",
    "plt.scatter(sensor_locations[1][0],sensor_locations[1][1], c = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Filtering\n",
    "We assume that we know initial point of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20000\n",
    "h = lambda particles, bearing: 90 - (torch.atan2(particles[:,1] - bearing.sensor_location[1],particles[:,0] - bearing.sensor_location[0]) * 57.295779513)\n",
    "initial_state = torch.Tensor(car_traj[0])\n",
    "initial_covarience_matrix = torch.diag(torch.Tensor([10**2,10**2,10,10]))\n",
    "initial_sampling = torch.distributions.MultivariateNormal(loc= initial_state,covariance_matrix = initial_covarience_matrix)\n",
    "particles = torch.squeeze(initial_sampling.expand([N, 1]).sample())\n",
    "weights = torch.ones(N) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_resample(weights):\n",
    "    \"\"\" Performs the residual resampling algorithm used by particle filters.\n",
    "    Based on observation that we don't need to use random numbers to select\n",
    "    most of the weights. Take int(N*w^i) samples of each particle i, and then\n",
    "    resample any remaining using a standard resampling algorithm [1]\n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : list-like of float\n",
    "        list of weights as floats\n",
    "    Returns\n",
    "    -------\n",
    "    indexes : ndarray of ints\n",
    "        array of indexes into the weights defining the resample. i.e. the\n",
    "        index of the zeroth resample is indexes[0], etc.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. S. Liu and R. Chen. Sequential Monte Carlo methods for dynamic\n",
    "       systems. Journal of the American Statistical Association,\n",
    "       93(443):1032â€“1044, 1998.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(weights)\n",
    "    #indexes = np.zeros(N, 'i')\n",
    "    indexes=torch.zeros(N,dtype=torch.int32)\n",
    "    # take int(N*w) copies of each weight, which ensures particles with the\n",
    "    # same weight are drawn uniformly\n",
    "    #num_copies = (np.floor(N*np.asarray(weights))).astype(int)\n",
    "    num_copies = (torch.floor(N*torch.as_tensor(weights))).int()\n",
    "    k = 0\n",
    "    for i in range(N):\n",
    "        for _ in range(num_copies[i]): # make n copies\n",
    "            indexes[k] = i\n",
    "            k += 1\n",
    "\n",
    "    # use multinormal resample on the residual to fill up the rest. This\n",
    "    # maximizes the variance of the samples\n",
    "    residual = weights - num_copies     # get fractional part\n",
    "    residual /= sum(residual)           # normalize\n",
    "    #cumulative_sum = np.cumsum(residual)\n",
    "    cumulative_sum = torch.cumsum(residual,dim=0)\n",
    "    cumulative_sum[-1] = 1. # avoid round-off errors: ensures sum is exactly one\n",
    "    indexes[k:N] = np.searchsorted(cumulative_sum, random(N-k))\n",
    "\n",
    "    return indexes\n",
    "\n",
    "\n",
    "\n",
    "def stratified_resample(weights):\n",
    "    \"\"\" Performs the stratified resampling algorithm used by particle filters.\n",
    "    This algorithms aims to make selections relatively uniformly across the\n",
    "    particles. It divides the cumulative sum of the weights into N equal\n",
    "    divisions, and then selects one particle randomly from each division. This\n",
    "    guarantees that each sample is between 0 and 2/N apart.\n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : list-like of float\n",
    "        list of weights as floats\n",
    "    Returns\n",
    "    -------\n",
    "    indexes : ndarray of ints\n",
    "        array of indexes into the weights defining the resample. i.e. the\n",
    "        index of the zeroth resample is indexes[0], etc.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(weights)\n",
    "    # make N subdivisions, and chose a random position within each one\n",
    "    #positions = (random(N) + range(N)) / N\n",
    "    positions = (torch.rand(N) + torch.arange(N)) / N\n",
    "\n",
    "    #indexes = np.zeros(N, 'i')\n",
    "    indexes=torch.zeros(N,dtype=torch.int32)\n",
    "    #cumulative_sum = np.cumsum(weights)\n",
    "    cumulative_sum = torch.cumsum(weights,dim=0)\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def systematic_resample(weights):\n",
    "    \"\"\" Performs the systemic resampling algorithm used by particle filters.\n",
    "    This algorithm separates the sample space into N divisions. A single random\n",
    "    offset is used to to choose where to sample from for all divisions. This\n",
    "    guarantees that every sample is exactly 1/N apart.\n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : list-like of float\n",
    "        list of weights as floats\n",
    "    Returns\n",
    "    -------\n",
    "    indexes : ndarray of ints\n",
    "        array of indexes into the weights defining the resample. i.e. the\n",
    "        index of the zeroth resample is indexes[0], etc.\n",
    "    \"\"\"\n",
    "    N = len(weights)\n",
    "\n",
    "    # make N subdivisions, and choose positions with a consistent random offset\n",
    "    #positions = (random() + np.arange(N)) / N\n",
    "    positions = (torch.rand(1) + torch.arange(N)) / N\n",
    "\n",
    "    #indexes = np.zeros(N, 'i')\n",
    "    indexes=torch.zeros(N,dtype=torch.int32)\n",
    "    #cumulative_sum = np.cumsum(weights)\n",
    "    cumulative_sum = torch.cumsum(weights,dim=0)\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes\n",
    "def update(particles, weights, bearing, sigma, h_func):\n",
    "    v = bearing.bearing - h_func(particles,bearing)\n",
    "    v[(v < -170) * (v > -190)] += 180 \n",
    "    v[(v < -350) * (v > -370)] += 360\n",
    "    v[(v < 190) * (v > 170)] -= 180\n",
    "    v[(v < 370) * (v > 350)] -= 360\n",
    "    weigths = weights * torch.distributions.normal.Normal(0,sigma).log_prob(v).exp()  \n",
    "    weights = weigths + torch.Tensor([1.e-300])     # avoid round-off to zero\n",
    "    weights = weigths / torch.sum(weights) # normalize\n",
    "    return weights\n",
    "def predict(particles, ts=torch.Tensor([1.])):\n",
    "    \"\"\" \n",
    "    One step propagation. It is linear in Bearing-Only-Tracking.\n",
    "    \"\"\"\n",
    "    particles = torch.matmul(particles,torch.t(torch.tensor(A(ts)).float())) \n",
    "    particles += torch.transpose(torch.mm(torch.Tensor(B(ts.numpy())),torch.empty([2,len(particles)]).normal_(mean=0,std=process_noise)),0,1)\n",
    "    return particles\n",
    "def estimate(particles, weights):\n",
    "    \"\"\"returns mean and variance of the weighted particles\"\"\"\n",
    "    mean = torch.sum(torch.mul(particles,weights[None].transpose_(0, 1)),dim=0)\n",
    "    var = torch.sum(torch.mul((particles - mean).pow(2),weights[None].transpose_(0, 1)),dim=0)\n",
    "    return mean, var\n",
    "def neff(weights):\n",
    "    return 1. / torch.dot(weights,weights)\n",
    "def resample_from_index(particles, weights, indexes):\n",
    "    particles[:] = particles[indexes.long()]\n",
    "    weights[:] = weights[indexes.long()]\n",
    "    weights = torch.full(weights.shape, 1.0 / len(weights))\n",
    "    return weights,particles\n",
    "def generate_video(particles, weights, bearings, video_name):\n",
    "    takip = []\n",
    "    old_time = bearings[0].time\n",
    "    plot = True\n",
    "    fig = plt.figure(figsize=(16,9))\n",
    "    camera = Camera(fig)\n",
    "    N = len(particles)\n",
    "    sel=0\n",
    "    for bearing in tqdm_notebook(bearings[1:]):\n",
    "        particles = predict(particles, ts = torch.Tensor([bearing.time - old_time]))\n",
    "        weights = update(particles, weights, bearing, sigma = measurement_sigma, h_func = h)\n",
    "        if neff(weights) < N/2:\n",
    "            indexes = systematic_resample(weights)\n",
    "            weights,particles=resample_from_index(particles, weights, indexes)\n",
    "\n",
    "        mean, var = estimate(particles, weights)\n",
    "        takip.append(mean)\n",
    "\n",
    "        if plot:\n",
    "                alpha = .20\n",
    "                if N > 5000:\n",
    "                    alpha *= np.sqrt(5000)/np.sqrt(N)           \n",
    "                plt.scatter(particles[:, 0], particles[:, 1], \n",
    "                            alpha=alpha, color='g')\n",
    "\n",
    "        plt.scatter(coordinates[0:sel,0], coordinates[0:sel,1], marker='+',color='y', s=180, lw=3)\n",
    "        plt.scatter(mean[0], mean[1], marker='+',color='r', s=180, lw=3)\n",
    "        plt.scatter(sensor_locations[0][0],sensor_locations[0][1], c = \"red\")\n",
    "        plt.scatter(sensor_locations[1][0],sensor_locations[1][1], c = \"red\")\n",
    "\n",
    "        if bearing.sensor_no==1:\n",
    "            slope=np.tan(np.deg2rad(-bearing.bearing-90))\n",
    "            y_slope=np.arange(3000,0,-10*slope)\n",
    "            x_slope=np.arange(3000,0,-10)\n",
    "            if len(y_slope)>len(x_slope):\n",
    "                plt.plot(x_slope,y_slope[0:len(x_slope)],alpha=0.2)\n",
    "            else:\n",
    "                plt.plot(x_slope[0:len(y_slope)],y_slope,alpha=0.2)\n",
    "\n",
    "        else:\n",
    "            slope=np.tan(np.deg2rad(bearing.bearing))\n",
    "            y_slope=np.arange(0,3000,10)\n",
    "            x_slope=np.arange(0,3000,10*slope)\n",
    "            if len(y_slope)>len(x_slope):\n",
    "                plt.plot(x_slope,y_slope[0:len(x_slope)],alpha=0.2)\n",
    "            else:\n",
    "                plt.plot(x_slope[0:len(y_slope)],y_slope,alpha=0.2)\n",
    "\n",
    "\n",
    "        old_time = bearing.time\n",
    "        camera.snap()\n",
    "        sel+=1\n",
    "\n",
    "    animation = camera.animate()\n",
    "    animation.save(video_name)\n",
    "    plt.close()\n",
    "    \n",
    "    Video(video_name)\n",
    "def one_target(particles, weights, bearings):\n",
    "    mean, var = estimate(particles, weights)\n",
    "    takip = [mean]\n",
    "    old_time = bearings[0].time\n",
    "    for bearing in tqdm(bearings[1:]):\n",
    "        N = len(particles)\n",
    "        particles = predict(particles, ts = torch.Tensor([bearing.time - old_time]))\n",
    "        weights = update(particles, weights, bearing, sigma = measurement_sigma, h_func = h)\n",
    "        # resample if too few effective particles\n",
    "        if neff(weights) < N/2:\n",
    "            indexes = systematic_resample(weights)\n",
    "#             print(\"--------------------\")\n",
    "#             print(weights)\n",
    "            weights,particles=resample_from_index(particles, weights, indexes)\n",
    "            \n",
    "\n",
    "        mean, var = estimate(particles, weights)\n",
    "        takip.append(mean)\n",
    "        old_time = bearing.time\n",
    "    \n",
    "    return torch.stack(takip)\n",
    "def performance(takip, car_traj):\n",
    "    ind_error = (takip - torch.Tensor(car_traj))[:,:2]\n",
    "    rmse = torch.sqrt(torch.mean(torch.sum(ind_error.pow(2), dim = 1)))\n",
    "    return rmse.data.numpy(), torch.sqrt(torch.sum(ind_error.pow(2), dim = 1)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "takip = one_target(particles, weights, bearings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_name = \"sa3.mp4\"\n",
    "# generate_video(particles, weights, bearings, video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo for different noise levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from itertools import product\n",
    "\n",
    "measurement_sigma = np.sqrt(1);\n",
    "process_noise = np.sqrt(0.5);\n",
    "sensor_locations = np.array([[0,0],[3000,3000]])\n",
    "ts = 1/2\n",
    "N = 20000\n",
    "h = lambda particles, bearing: 90 - (torch.atan2(particles[:,1] - bearing.sensor_location[1],particles[:,0] - bearing.sensor_location[0]) * 57.295779513)\n",
    "\n",
    "def parallel_helper(a):\n",
    "    print(a)\n",
    "    car_traj, bearings, coordinates = generate_a_target(measurement_sigma, process_noise, ts)\n",
    "    initial_state = torch.Tensor(car_traj[0])\n",
    "    initial_covarience_matrix = torch.diag(torch.Tensor([10**2,10**2,10,10]))\n",
    "    initial_sampling = torch.distributions.MultivariateNormal(loc= initial_state,covariance_matrix = initial_covarience_matrix)\n",
    "    particles = torch.squeeze(initial_sampling.expand([N, 1]).sample())\n",
    "    weights = torch.ones(N) / N\n",
    "    takip = one_target(particles, weights, bearings)\n",
    "    return [performance(takip, car_traj)]\n",
    "\n",
    "with multiprocessing.Pool(processes=5) as pool:\n",
    "    results = pool.map(parallel_helper, list(range(1,5)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
